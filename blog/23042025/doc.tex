\documentclass[a4paper]{article}

\usepackage[margin=1.5in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{xcolor}

%\usepackage{newtxtext,newtxmath}
%\usepackage{tgtermes}
%\usepackage[italic]{mathastext}

\usepackage{enumitem}
%\usepackage{hyperref}
%\usepackage{graphicx}
%\usepackage{quantikz2}

\DeclareMathOperator{\N}{\mathbf{N}}
\DeclareMathOperator{\Z}{\mathbf{Z}}
\DeclareMathOperator{\Q}{\mathbf{Q}}
\DeclareMathOperator{\R}{\mathbf{R}}
\DeclareMathOperator{\C}{\mathbf{C}}
\DeclareMathOperator{\F}{\mathbf{F}}
\DeclareMathOperator{\Tr}{Tr}

\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem*{sol}{Solution}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}

\title{Von Neumann's impossiblity proof}
\author{Ernesto Camacho}
\begin{document}
    \maketitle


    In his 1932 book \textit{The Mathematical Foundations of
    Quantum Mechanics}, when discussing the topic of
    incompatible measurements (section?), John von Neumann
    gives what is coloquially known as an
    \textit{impossibility proof} of hidden variable models
    for quantum mechanics.  It's now a well accepted fact
    that von Neumann's proof while not being both physically
    conclusive nor correctly interpreted, was used to great
    avail to limit investigation into any theory that could
    explain the statistical nature of quantum theory, that
    is until John S. Bell\footnote{It is also an important
        part of this story now a days to highlight that
        Greta H... identified this mistake much earlier, in
        19.., but on account of multiple reasons, was
    virtually ignored.} identified a serious shortcoming of von
    Neumann's argument, which upon further analysis then led
    to his famous results.

    This shortcoming has been interpreted as a mistake and
    ``foolish'' by Bell himself, and this has since become
    the consensus. The problem rests upon an assumption made
    on the functional relations of expectation values and
    what that implies for any valuation of observables.
    While reading Bell's 1966 paper where he discusses this
    shortcoming, I was convinced by his criticism as it
    seemed physically very natural, and so I was amazed that
    von Neumann of all people would make such such an
    oversight, so much so that I began to think that maybe
    Bell was over-interpreting the result and possibly
    misunderstood the argument. It turns out that this
    feeling is not only mine, as a couple of authors (who?
    Lubos)  believe that Bell, Greta and the mayority of the
    physics community has actually misunderstood von
    Neumann.

    %Some argue that there is no impossibility proof
    %to begin with (who?)! In his book Johny just outlines a
    %couple of logical consequences that follow his careful
    %analysis of compatible and incompatible measurements,
    %and in no sense did he explicitly state that these
    %arguments proved the non-existance of a deeper theory.
    %Others (more interestingly) believe that von Neumann had
    %stumbled upon the notion of contextuality already in the
    %beginings of quantum theory.

    The lack of cohesive agreement between authoriative
    figures (David Mermin and Schack have since replied that
    Bun etc have misunderstood Bell and Greta), which has
    created this back and forth discussion between multiple
    authors each claiming the other is missing the big
    picture is a bit frustratring.  And so not wanting to
    draw any conclusions directly from these reviews, I have
    decided to look into the proof myself and see what can
    be said about it in the context of John S. Bell's
    criticism.

    \section{von Neumann's proof}

    The proof can be found in chapter IV \textit{Deductive
    Development of the Theory}, between pages 295-306 from
    the original english translation. 

    \begin{enumerate}
        \item Everything which can be said about the state
            of a system must be derived from its wave
            function $\phi$.
        \item This does not restrict us to the stationary
            states of the system.
        \item What pronouncements can now be made regarding
            a system which is in the state $\phi$?
        \item All assertions of quantum mechanics can be
            reduced to the statistical formula 
            \begin{equation}
                \text{Exp}(\mathcal{R},\phi)
                = (\phi, R\phi),
            \end{equation}
            which is the expectation value of the quantity
            $\mathcal{R}$ in the state $\phi$, and $R$ the
            corresponding operator of $\mathcal{R}$.
        \item Such a formula can be derived from a few
            general qualitative assumptions.
        \item Forget the whole of quantum mechanics but
            retain the following:
            \begin{enumerate}
                \item Suppose a system $\mathsf{S}$ is
                    given, which is characterized for the
                    experimenter by the enumeration of all
                    the effectively measureable quantities
                    in it and their functional realtions
                    with one another.
                \item Each quantity includes the directions
                    as to how it is to be measured---and how
                    its value is to be read or calculated
                    from the indicator positions on the
                    measuring instruments.
                \item If $\mathcal{R}$ is a quantity and
                    $f(x)$ any (measurable function), then
                    the quantity $f(\mathcal{R})$ is defined
                    as follows: to measure $f(\mathcal{R})$,
                    we measure $\mathcal{R}$ and find the
                    value $a$. Then $f(\mathcal{R})$ has the
                    value $f(a)$. 
                \item All quantities $f(\mathcal{R})$ with
                    $\mathcal{R}$ fixed, are measured
                    simultaneously with $\mathcal{R}$.
                \item \textbf{def} Two (or more) quantities
                    $\mathcal{R}, \mathcal{S}$ are
                    simultaneously measurable if there is an
                    arrangement which measures both
                    simultaneously in the same
                    system---except that their respective
                    values are to be calculated in different
                    ways from the readings.
                \item For such quantities and a measurable
                    function $f(x,y)$, we can also define
                    the quantity
                    $f(\mathcal{R},\mathcal{S})$. This is
                    measured if we measure
                    $\mathcal{R},\mathcal{S}$ 
                    simultaneously---if the values $a,b$ are
                    found for these, then the value of
                    $f(\mathcal{R},\mathcal{S})$ is
                    $f(a,b)$. 
                    \begin{quote}
                        But it should be realized that it is
                        completely meaningless to try to
                        form $f(\mathcal{R},\mathcal{S})$ if
                        $\mathcal{R},\mathcal{S}$ are not
                        simultaneously measurable: there is
                        no way of giving the corresponding
                        measuring arrangement.
                    \end{quote}
                \item The investigation of the physical
                    quantities related to a single object
                    $\mathsf{S}$ is not the only thing which
                    can be done---especially if doubts
                    exists relative to the simultaneous
                    measurability of several quantities. In
                    such cases it is also possible to
                    observe great statistical ensembles
                    which consist of many systems
                    $\mathsf{S}_1,\ldots,\mathsf{S}_N$. 
                \item In such an ensemble we do not measure
                    the \textit{value} of a quantity
                    $\mathcal{R}$ but its distribution of
                    values: i.e., for each interval $a' < a
                    \leq a''$ the number of those among the
                    $\mathsf{S}_1,\ldots,\mathsf{S}_N$ for
                    which the value of $\mathcal{R}$ lies in
                    the interval---dividing this number by
                    $N$ we obtain the probability function
                    \begin{equation}
                        w(a',a'')
                        = w(a'') - w(a'),
                    \end{equation}
                    where $w(a')$ is the probability of $a
                    \leq a'$, i.e., it belongs to the
                    interval $(-\infty,a')$. In quantum
                    mechanics, this is the spectral measure
                    given by the observable $\mathcal{R}$,
                    which we know and love, i.e., if
                    $E(\lambda)$ is the resolution of the
                    identity (as Johnny calls it)
                    corresponding to $R$ (it is the spectral
                    PVM of $\mathcal{R}$), then
                    \begin{equation}
                        w_{\mathcal{R}}(a)
                        = \|E(a)\phi\|^2
                        = (\phi,E(a)\phi).
                    \end{equation}
                \item The essential advantages of the
                    observation of such ensembles are these:
                    \begin{enumerate}
                        \item Even if the measurement of a
                            quantity $\mathcal{R}$ should
                            alter the measured system
                            $\mathsf{S}$ to an important
                            degree, the statistical
                            determination of the probability
                            distribution of $\mathcal{R}$ in
                            the ensemble
                            $\mathsf{S}_1,\ldots,\mathsf{S}_N$ 
                            will alter this ensemble
                            arbitrarily little if $N$ is
                            sufficiently large. {\color{blue}
                            I think all Johnny is saying is
                            that we can infer the
                            expectation value as long as $N$
                            is large enough?}
                        \item Even if two (or more)
                            quantities
                            $\mathcal{R},\mathcal{S}$ in a
                            single system $\mathsf{S}$ are
                            not simultaneously measurable,
                            their probability distributions
                            in a given ensemble
                            $\mathsf{S}_1,\ldots,\mathsf{S}_N$ 
                            can be obtained with arbitrary
                            accuracy if $N$ is sufficiently
                            large. {\color{blue} This
                            essentially means that after
                            splitting the ensemble, if the
                            sub-ensembles are small enough
                            the disturbances will be
                            neglegible and one can say that
                            both quantities have been
                            measured for the same ensemble.}
                    \end{enumerate}
            \item von Neumann notes that the
                introduction of the statistical
                ensembles, i.e., of probability methods,
                is undertaken because of the possbility
                of affecting a single system by
                measurement, and by the possibliity of
                non-simultaneous measurability of
                serveral quantities.
            \item For such ensembles, it is not
                surprising that a physical quantity
                $\mathcal{R}$ does not have a sharp
                value, i.e., that its distribtuion
                function does not consist of a single
                value $a_0$, but that several values or
                intervals are possible, and that a
                positive dispersion exists. However, two
                different reasons for this behaviour are
                a priori conceivable:
                \begin{enumerate}
                    \item The individual systems
                        $\mathsf{S}_1,\ldots,\mathsf{S}_N$ 
                        of our ensemble can be in
                        different states, so that the
                        ensemble
                        $\mathsf{S}_1,\ldots,\mathsf{S}_N$ 
                        is defined by their relative
                        frequencies. The fact that we do
                        not obtain sharp values for the
                        phsyical quantities in this case
                        is caused by our lack of
                        information: we don't know what
                        state we are measuring, and
                        therefore we cannot predict the
                        results.
                    \item All individual systems
                        $\mathsf{S}_1,\ldots,\mathsf{S}_N$ 
                        are in the same state, but the
                        laws of nature are not causal.
                        Then the cause of the
                        dispersions is not our lack of
                        information, but is nature
                        itself, which has disregarded
                        the ``principle of sufficient
                        cause''.
                \end{enumerate}
            \item Here now Johnny states his opinion:
                The first case alluded to (case I) is
                well-known, while the second (case II)
                is new. To be sure we shall be sceptical
                at first of the possibility of its
                existence, but we shall find an
                objective criterion which allows us to
                distinguish between its appearance or
                non-appearance. It appears at first that
                serious objections can be raised against
                its conceivability and its
                meaningfulness, be we believe that these
                objections are not valid, and that
                certain difficulties permit no other way
                out but case II.
            \item One might object againts II that nature
                cannot violate the principle of sufficient
                cause, i.e., causality, at all, because this
                is merely a definition of identity. That is,
                the theorem that two identical objects
                $\mathsf{S}_1,\mathsf{S}_2$---i.e., two
                replicas of the system $\mathsf{S}$ which
                are in the same state---will remain
                identical in all conceivable interactions is
                true because it is tautological. What this
                means that if both systems could react
                differently to the same intervention in
                their interaction, then we would not have
                called them identical. So for an ensemble
                $\mathsf{S}_1,\ldots,\mathsf{S}_N$, which
                has dispersion relative to a quantity
                $\mathcal{R}$, the individual systems
                $\mathsf{S}_1,\ldots,\mathsf{S}_N$ cannot
                all be in the same state. {\color{blue}
                    Applied to quantum mechanics this means
                    that the wave function (when it is not
                    an eigenfunction) must be
                \textit{incomplete}.} Therefore other
                variables must exist, the \textit{hidden
                parameters} (mentioned in III.2?).
            \item In a large statistical ensemble therefore,
                as long as any quantity $\mathcal{R}$ 
                possesses a dispersion in it, the
                possibility must exist of resolving it into
                several differently constituted parts. This
                is all the more plausible, since ordinarily
                a simple method of such a rseolution seems
                to exist: we can resolve it according to the
                various values which $\mathcal{R}$ has in
                the ensemble. After a subdivision or
                resolution relative to all quantities
                $\mathcal{R},\mathcal{S},
                \mathcal{T},\ldots$ which are carried out, a
                truly homogenous ensemble would then be
                obtained. At the end of the process, these
                quantities would have no further dispersion
                in any of the sub-ensembles.
            \item {\color{blue} Hold up.} The statements
                contained in the last sentences are
                incorrect because we did not consider the
                fact that measurement changes the measured
                system....
            \item In the atom we are at the boundary of the
                physical world, where each measurement is an
                interference of the same order of magnitude
                as the object measured, and therefore
                affects it basically. Thus the uncertainty
                relations are at the root of these
                difficulties.
            \item {\color{blue} Here Johnny sets up a
                    question about hidden parameters that
                could be answered.} As we see, the attempt
                to interpret causality as an equality
                definition led to a question of fact which
                can and must be answered, and which might
                conceivaby answered negatively. This is the
                question: is it really possible to represent
                each ensemble
                $\mathsf{S}_1,\ldots,\mathsf{S}_N$, in which
                there is a quantity $\mathcal{R}$ with
                dispersion, by the superposition of two (or
                more) ensembles different from one another
                and from it?

                If $\mathsf S_1,\ldots,\mathsf S_N$ were the
                mixture of $\mathsf S_1',\ldots,\mathsf
                S_P'$ and $\mathsf S_1'',\ldots,\mathsf
                S_Q''$ , the probability function
                $w_{\mathcal{R}}(a)$ could be expressed with
                the aid of the probability functions
                $w_{\mathcal{R}}'(a)$ and
                $w_{\mathcal{R}}''(a)$ of the two ensembles,
                \begin{equation}
                    w_{\mathcal{R}}(a)
                    = \alpha w_{\mathcal{R}}'(a) + \beta
                    w_{\mathcal{R}}''(a),
                \end{equation}
                where $\alpha = P / N, \beta = Q/N$ and $P +
                Q = N$ are independent of $\mathcal{R}$. If
                iin an ensemble with the probability
                functions $w_\mathcal{R}(a)$ there exist
                quantities $\mathcal{R}$ with dispersion,
                are there two other ensembles with the
                probability functions $w_{\mathcal{R}}'(a)$ 
                and $w_{\mathcal{R}}''(a)$ respectively,
                such that for all $\mathcal{R}$ the
                additivity relation above holds?

                This can be expressed in a different manner
                if we characterize an ensemble not by the
                probability functions $w_{\mathcal{R}}(a)$ 
                of the quantity $\mathcal{R}$ but by their
                expectation values
                \begin{equation}
                    \text{Exp}(\mathcal{R})
                    = \int_{-\infty}^{\infty} a \,
                    \text{dw}_{\mathcal{R}}(a).
                \end{equation}
                Then our questions is the following: An
                ensemble is dispersion free if in it, for
                each $\mathcal{R}$, 
                \begin{equation}
                    \text{Exp}([\mathcal{R} -
                    \text{Exp}(\mathcal{R})]^2)
                    = \text{Exp}(\mathcal{R}^2) -
                    [\text{Exp}(\mathcal{R})]^2
                \end{equation}
                is equal to zero. If this is not the case,
                is it always possible to find two other
                ensembles with
                \begin{equation}
                    \text{Exp}'(\mathcal{R}),
                    \quad\text{and}\quad
                    \text{Exp}''(\mathcal{R})
                    \quad\text{with}\quad
                    \text{Exp}(\mathcal{R}) \neq
                    \text{Exp}'(\mathcal{R}) \neq
                    \text{Exp}''(\mathcal{R})
                \end{equation}
                such that
                \begin{equation}
                    \text{Exp}(\mathcal{R})
                    = \alpha \text{Exp}'(\mathcal{R}) +
                    \beta \text{Exp}''(\mathcal{R}),
                    \quad \alpha > 0, \beta > 0, \alpha +
                    \beta = 1
                \end{equation}
                always hold?\footnote{For a single quantity
                    $\mathcal{R}$, the number
                    $\text{Exp}(\mathcal{R})$ is not a
                    substitute for the function
                    $w_{\mathcal{R}}(a)$ ; on the other
                    hand, the knowledge of all
                    $\text{Exp}(\mathcal{R})$ is equivalent
                    to the knowledge of all
                $w_{\mathcal{R}}(a)$ } To handle this
                question mathematically, it is preferable
                not to consider the ensembles $\mathsf
                S_1,\ldots,\mathsf S_N$ themselves, but
                rather the corresponding
                $\text{Exp}(\mathcal{R})$. To each ensemble
                there belongs one such function which is
                defined for all physical quantities
                $\mathcal{R}$ in $\mathsf S$, and which
                takes on real numbers as values, and which
                conversly, completely characterizes the
                ensemble in all its statistical properties.
            \item We must still find out which properties an
                $\mathcal{R}$ function must possess in order
                that it be the $\text{Exp}(\mathcal{R})$ of
                a suitable ensemble. As soon as this is
                done, we can \textit{define}:
                \begin{enumerate}
                    \item an $\mathcal{R}$ function, which
                        is an $\text{Exp}(\mathcal{R})$, is
                        said to be dispersion free if it
                        satisfies the condition above.
                    \item An $\mathcal{R}$ function, which
                        is an $\text{Exp}(\mathcal{R})$, is
                        said to be homogeneous or pure if,
                        for it, the additivity relation
                        implies that
                        \begin{equation}
                            \text{Exp}(\mathcal{R})
                            = \text{Exp}'(\mathcal{R})
                            = \text{Exp}''(\mathcal{R}).
                        \end{equation}
                \end{enumerate}
            \item It is conceptually plausible that each
                dispersion free
                $\text{Exp}(\mathcal{R})$-function should be
                pure, and we shall soon prove it. But our
                question for the moment is the converse one:
                is each pure
                $\text{Exp}(\mathcal{R})$-function
                dispersion free?
            \item Johnny states that it is evident that each
                $\text{Exp}(\mathcal{R})$-function must
                possess the following properties:
                \begin{enumerate}
                    \item If the quantity $\mathcal{R}$ is
                        identically $1$ (i.e., if the
                        directions for measurmeent are: no
                        measurement is necessary, because
                        $\mathcal{R}$ always ahs the value
                        $1$), then $\text{Exp}(\mathcal{R})
                        = 1$.
                    \item For each $\mathcal{R}$ and each
                        real number $a$, $\text{Exp}(a
                        \mathcal{R}) = a
                        \text{Exp}(\mathcal{R})$.
                    \item If the quantity $\mathcal{R}$ is
                        by nature non-negative, then also
                        $\text{Exp}(\mathcal{R}) \geq 0$.
                    \item If the quantities
                        $\mathcal{R},\mathcal{S},\ldots$ are
                        simultaneously measurable, then
                        $\text{Exp}(\mathcal{R} +
                        \mathcal{S} + \ldots) =
                        \text{Exp}(\mathcal{R}) +
                        \text{Exp}(\mathcal{S}) + \ldots.$
                        {\color{blue} Importantly, von
                        Neumann states that} For
                        non-simultaneously measurable
                        quantities $\mathcal{R},
                        \mathcal{S},\ldots$, the quantity
                        $\mathcal{R} + \mathcal{S} + \ldots$ 
                        is undefined.
                \end{enumerate}
                All this follows immediately from the
                definitions of the quantities under
                consideration (i.e., the directions for
                their measurement), and from the definition
                of the expectation value as the arithmetic
                mean of all results of measurement in a
                sufficiently large statistical ensemble.
            \item Regarding the fourth property
                ({\color{blue} property D in his book and
                the main source of dispute}), it should be
                noted that its correctness depends on this
                theorem on probability: the expectation
                value of a sum is always the sum of the
                expectation values of the individual terms,
                independent of whether probability
                dependencies exists between these or not.
                That we have formulated it only for
                simultaneously measurable
                $\mathcal{R},\mathcal{S},\ldots$ is natural,
                since otherwise $\mathcal{R} + \mathcal{S} +
                \ldots$ is meaningless.
            \item But the algorithm of quantum mechanics
                contains still another operation, which goes
                beyond the one just discussed: namely, the
                addition of two arbitrary quantities, which
                are not necessarily simultaneously
                observable. This oepration depends on the
                fact that for two Hermitian operators,
                $R,S$, the sum $R + S$ is also an Hermitian
                operator, even if the $R,S$ do not commute,
                while, for example, the product $RS$ is
                again Hermitian only in the event of
                commutativity. In each state $\phi$ the
                expectation values behave additively: 
                \begin{equation}
                    (\phi,(R+S)\phi)
                    = (\phi, R\phi) + (\phi,S\phi).
                \end{equation} 
                He now incorporates this fact into the
                general set-up which at this point has not
                yet specialized to quantum mechanics:
                \begin{enumerate}
                    \item If $\mathcal{R},\mathcal{S}$ are
                        arbitrary quantities, then there is
                        an additional quantity $\mathcal{R}
                        + \mathcal{S} + \ldots$ (which does
                        not depend on the choice of the
                        $\text{Exp}(\mathcal{R})$-function),
                        such that $\text{Exp}(\mathcal{R} +
                        \mathcal{S} + \ldots) =
                        \text{Exp}(\mathcal{R}) +
                        \text{Exp}(\mathcal{S}) + \ldots$.
                \end{enumerate}
                In general (that is for not necessarily
                commuting observables), the sum is
                characterized by linearity only in an
                implicit way, and it shows no way to
                construct from the measurement directions
                for $\mathcal{R},\mathcal{S},\ldots$ such
                directions for $\mathcal{R} + \mathcal{S} +
                \ldots$.\footnote{This is of course the
                    bread and butter of quantum mechanics.
                    For example, the energy operator of the
                    Heisenberg theory of an electron moving
                    in a potential field $V(x,y,z)$,
                    \begin{equation}
                        H
                        = \frac{(P^{X})^2 + (P^{Y})^2 +
                        (P^{Z})^2}{2m} +
                        V(Q^{X},Q^{Y},Q^{Z}),
                    \end{equation}
                    is a sum of two non-commuting operators
                    \begin{equation}
                        R =  \frac{(P^{X})^2 + (P^{Y})^2 +
                        (P^{Z})^2}{2m},
                        \quad
                        S = V(Q^{X},Q^{Y},Q^{Z}).
                    \end{equation}
                    While the measurement of the quantity
                    $\mathcal{R}$ belonging to $R$ is a
                    momentum measurement, and that of the
                    quantity $\mathcal{S}$ belonging to $S$ 
                    is a coordinate measurement, we measure
                    the quantity $\mathcal{R} + \mathcal{S}$ 
                    belonging to $H = R + S$ in an entirely
                    different way: for example, by the
                    measurement of the frequency of the
                    spectral lines emitted by this (bound)
                    electron, since these lines determine
                    (by reason fo the Bohr frequency
                    relation) the energy levels, i.e., the
                    $\mathcal{R} + \mathcal{S}$ values.
                    Nevertheless, under all circumstances,
                    \begin{equation}
                        \text{Exp}(\mathcal{R} +
                        \mathcal{S})
                        = \text{Exp}(\mathcal{R}) +
                        \text{Exp}(\mathcal{S}).
                    \end{equation}
                }
            \item After analysis of the nuance of relative
                probabilities, Johnny arrives at the
                following form for the previous conditions:
                \begin{enumerate}
                    \item IF the quantity $\mathcal{R}$ is
                        by nature non-negative, for exmaple,
                        if it is the square of another
                        quantiy $\mathcal{S}$, then
                        $\text{Exp}(\mathcal{R}) \geq 0$.
                    \item If
                        $\mathcal{R},\mathcal{S},\ldots$ are
                        arbitrary quantities and
                        $a,b,\ldots$ real numbers, then
                        $\text{Exp}(a \mathcal{R} + b
                        \mathcal{S} + \ldots) = a
                        \text{Exp}(\mathcal{R}) + b
                        \text{Exp}(\mathcal{S}) + \ldots$.
                \end{enumerate}
            \item With all of this in hand, we are now in
                the position to make a decision on the
                question of causality, as soon as we know
                the physical quantities in $\mathsf S$, as
                well as the functional relations existing
                among them.
        \end{enumerate}
        \item  We now come to section 2 of this chapter,
            \textit{Proof of the Statistical Formulas}.
            There corresponds to each physical quantity of a
            quantum mechanical system, a unique hypermaximal
            Hermitian operator, and it is convenient to
            assume that this correspondence is
            one-to-one.\footnote{How would dropping the
            assumption affect the argument?} In such a case
            the following rules are valid
            \begin{enumerate}
                \item If the quantity $\mathcal{R}$ has the
                    operator $R$, then the quantity
                    $f(\mathcal{R})$ has the operator
                    $f(R)$.
                \item If the quantities
                    $\mathcal{R},\mathcal{S},\ldots$ have
                    the operators $R,S,\ldots$, then the
                    quantity $\mathcal{R} + \mathcal{S} +
                    \ldots$ has the operator $R + S +
                    \ldots$. (The simultaneous measurability
                    of $\mathcal{R},\mathcal{S},\ldots$ is
                    not assumed.
            \end{enumerate}
            Now begins his proof. Let $\phi_1,\phi_2,\ldots$ 
            be a complete orthonormal set. In place of each
            operator $R$ let us consider the matrix
            $a_{\mu\nu} = (\phi_\mu, R\phi_v)$. We form the
            Hermitian operators with the respective matrices
            \begin{align}
                e_{\mu\nu}^{(n)}
                &= \begin{cases}
                    1 & \mu = \nu = n, \\
                    0 & \text{otherwise},
                \end{cases} \\
                f_{\mu\nu}^{(mn)}
                &= \begin{cases}
                    1 & \mu = m, \nu = n, \\
                    1 & \mu = n, \nu = m, \\
                    0 & \text{otherwise},
                \end{cases} \\
                g_{\mu\nu}^{(mn)}
                &= \begin{cases}
                    i & \mu = m, \nu = n \\
                    -i & \mu = n, \nu = m \\
                    0 & \text{otherwise},
                \end{cases}
            \end{align}
            these operators are
            \begin{equation}
                P_{[\phi_n]},
                \quad
                P_{[(\phi_m+\phi_n) / \sqrt{2}]},
                - P_{[(\phi_m-\phi_n) / \sqrt{2}]},
                \quad
                P_{[(\phi_m + i\phi_n) / \sqrt{2}]} -
                P_{[(\phi_m - i\phi_n) / \sqrt{2}]},
            \end{equation}
            let the corresponding quantities be
            $\mathcal{U}^{(n)}$, $\mathcal{V}^{(mn)}$,
            $\mathcal{W}^{(mn)}$. Evidently
            \begin{equation}
                a_{\mu\nu}
                = \sum_{n}^{} a_{nn} e_{\mu\nu}^{(n)}
                + \sum_{m,n, m < n}^{} \text{Re }
                a_{mn}f_{\mu\nu}^{(mn)} + \sum_{m,n,m<n}^{}
                \text{Im } a_{mn} g_{\mu\nu}^{(mn)},
            \end{equation}
            therefore
            \begin{equation}
                \mathcal{R}
                = \sum_{n}^{} a_{nn} \mathcal{U}^{(n)} +
                \sum_{m,n,m<n}^{} \text{Re } a_{mn}
                \mathcal{V}^{(mn)} + \sum_{m,n,m<n}^{}
                \text{Im } a_{mn} \mathcal{W}^{(mn)},
            \end{equation}
            and
            \begin{equation}
                \text{Exp}(\mathcal{R})
                = \sum_{n}^{} a_{nn}
                \text{Exp}(\mathcal{U}^{(n)}) +
                \sum_{m,n,m<n}^{} \text{Re } a_{mn}
                \text{Exp}(\mathcal{V}^{(mn)}) + \sum_{m,n,m<n}^{}
                \text{Im } a_{mn}
                \text{Exp}(\mathcal{W}^{(mn)}).
            \end{equation}
            Therefore if one sets
            \begin{align}
                \mu_{nn}
                &= \text{Exp}(\mathcal{U}^{(n)}) \\
                \mu_{mn}
                &= \frac{1}{2}
                \text{Exp}(\mathcal{V}^{(mn)}) + \frac{i}{2}
                \text{Exp}(\mathcal{W}^{(mn)}) \\
                \mu_{nm}
                &= \frac{1}{2}
                \text{Exp}(\mathcal{V}^{(mn)}) - \frac{i}{2}
                \text{Exp}(\mathcal{W}^{(mn)}),
            \end{align}
            then
            \begin{equation}
                \text{Exp}(\mathcal{R})
                = \sum_{m,n}^{} \mu_{nm} a_{mn}.
            \end{equation}
            Since $\mu_{nm} = \mu_{nm}$ we can define a
            Hermitian operator $U$ by $(\phi_m, U\phi_n) =
            \mu_{mn}$ and the right side is the $\Tr(UR)$.
            Therefore we obtain the formula
             \begin{equation}
                \text{Exp}(\mathcal{R})
                = \Tr(UR),
            \end{equation}
            where $U$ is a Hermitian operator indepdnent of
            $R$ and therefore is determined by the ensemble
            itself. The trace satisfies the linearity
            condition of course. He goes on to show that it
            also satisfies the non-negativity condition
            mentioned above.
        \item Hence we have determined the functions
            $\text{Exp}(\mathcal{R})$ completely; they
            corerspond to the definite Hermitian operator
            $U$, and the connection is given by $\Tr$. We
            shall call $U$ the statistical oeprator of the
            ensemble under consideration. {\color{blue}
                Wait, is this where he defined the
                statistical operator know the as the density
            matrix?}
        \item He now investigates the conditions, i.e.,
            discover the dispersion free and homogeneous
            ensembles among the $U$. We focus on the
            homogenous case. $U$ is homoegenous if from 
            \begin{equation}
                U = V + W
            \end{equation}
            it follows that $V = c'U, W = c''U$. We assert
            that this property holds for $U = P_{[\phi]}$ 
            and \textit{only} for these.
    \end{enumerate}

    \section{Golub}

    Main conclusions of von Neumann's analysis of hidden
    parameters:
    \begin{enumerate}
        \item Hidden variables cannot be incorporated into
            the existing theory of quantum mechanics without
            major modifications to the fundamental theory,
        \item and that if they did exist, quantum mechanics
            would have already failed in situations where it
            has been successfully applied.
    \end{enumerate}

    The authors mention that since D. Bohm's theory has no
    experimental consequences, we might assume, wrongly or
    rightly, that von Neumann did not entertain the notion
    of a modification of quantum mechanics with no testable
    consequences. To the author's knowledge, a successful
    hidden variable extension to quantum mechanics with
    testable consequences has not yet been produced, leading
    us to conclude that von Neumann's analysis is worthy of
    rehabilitation.

    \begin{quote}
        In MFQM von Neumann showed that adding hiddden
        parameters to the existing theory leads to a logical
        contradiction. His analysis assumes the existence of
        hidden variables, and then shows that it would be
        possible to construct dispersion-free coordinate
        states, after showing that such states are not
        possible within the existing Hilbert space framework
        of quantum mechanics. He concluded that the present
        quantum theory would have laready given false
        predictions if hidden variables existed.
        \textit{However, he left open the possibility of
        hidden parameters while recognizing that they would
        require a vastly modified theory}.
    \end{quote}

    To be clear, von Neumann put considerable effor tinto
    the discussion of the possibility that the statistical
    behavior associated with the quantum states might be due
    to fluctuations in some unknown parameters, whose
    variations would lead to random behavior.

    Von Neumann considers quantum mechanics to be
    characterized by the realtion for the expectation value
    of a physical variable reprsented by a Hermitian
    operator $R$, in some state $\phi$:
    \begin{equation}
        \text{Exp}(R)
        = (\phi, R\phi).
    \end{equation}
    von Neumann does not assume this to be an
    axiom/assumptions, but replaces it by a set of
    assumptions which can lead to the calculation
    \begin{equation}
        \text{Exp}(R)
        = \Tr(\rho R).
    \end{equation}
    Without specifying a definite method for calculating
    expectation values, he listed several properties he
    would expect to be features of such a method. Most
    importantly for this discussion, is the linearity of the
    expectation value.

    von Neumann then derives the trace rule from the
    linearity of the expectation value (with other
    assumptions at play).

    If the statistical variations in experimental results
    were due to averaging over unknown hidden variables, the
    ensembles described by quantum states would consist of
    separate subensembles, each with some exat value of all
    physical variables.

    \section{Bell's criticism}

    .

\end{document}
